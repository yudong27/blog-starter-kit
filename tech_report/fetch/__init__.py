import requests
import os
from typing import List

from loguru import logger
from formatter import DataFetchFormatter

# ================= é…ç½®åŒº =================
# 1. Reddit é…ç½®
SUBREDDIT = "technology"  # ä½ å¯ä»¥æ¢æˆ MachineLearning, LocalLLaMA ç­‰ç¡¬æ ¸æ¿å—
LIMIT = 3                 # æ¯æ¬¡æŠ“å–å‰ 3 æ¡æµ‹è¯•
# âš ï¸ æå…¶é‡è¦ï¼šå¿…é¡»ä¼ªè£… User-Agentï¼Œå¦åˆ™ä¼šè¢« Reddit 100% æ‹¦æˆªï¼
HEADERS = {
    "User-Agent": "TechReporterBot/1.0 (by /u/dan)" 
}


def fetch_reddit_top_posts():
    """ä» Reddit è·å–ä»Šæ—¥æœ€çƒ­æ–°é—»"""
    print(f"ğŸ“¡ æ­£åœ¨æ½œå…¥å‰çº¿ r/{SUBREDDIT} è·å–æœºå¯†æƒ…æŠ¥...")
    url = f"https://www.reddit.com/r/{SUBREDDIT}/top.json?limit={LIMIT}&t=day"
    
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ è·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return []
    
    posts = []
    data = response.json()
    for item in data['data']['children']:
        post_data = item['data']
        posts.append({
            "title": post_data['title'],
            "url": post_data['url']
        })
    return posts

def fetch_hackernews_top_posts():
    """ä» Hacker News è·å–ä»Šæ—¥æœ€çƒ­ç¡¬æ ¸æ–°é—»"""
    print("ğŸ“¡ æ­£åœ¨æ½œå…¥å‰çº¿ Hacker News è·å–æœºå¯†æƒ…æŠ¥...")
    
    # æ­¥éª¤ 1ï¼šè·å–çƒ­é—¨æ–‡ç« çš„ ID åˆ—è¡¨
    top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    
    # å¦‚æœä½ åœ¨å›½å†…ä¸å¼€ä»£ç†è¿è¡Œï¼Œå¯èƒ½ä¼šå¡ä½ã€‚
    # å»ºè®®ä¿æŒä½ çš„ä»£ç†è½¯ä»¶å¼€å¯ï¼ˆé€šå¸¸ requests åº“ä¼šè‡ªåŠ¨èµ°ç³»ç»Ÿå…¨å±€ä»£ç†ï¼‰ã€‚
    try:
        response = requests.get(top_stories_url, timeout=10)
        if response.status_code != 200:
            print(f"âŒ è·å– ID åˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
            
        story_ids = response.json()[:LIMIT] # å–å‰ 3 ä¸ªæ–°é—»çš„ ID
        
        posts = []
        # æ­¥éª¤ 2ï¼šæ ¹æ® ID è·å–å…·ä½“æ–°é—»çš„æ ‡é¢˜å’Œé“¾æ¥
        for story_id in story_ids:
            item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            item_resp = requests.get(item_url, timeout=10).json()
            
            # è¿‡æ»¤æ‰æ²¡æœ‰ URL çš„è®¨è®ºå¸–
            if 'url' in item_resp and 'title' in item_resp:
                posts.append({
                    "title": item_resp['title'],
                    "url": item_resp['url']
                })
        return posts
        
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}\n(ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿ä½ çš„ä»£ç†è½¯ä»¶å¤„äº'å…¨å±€è·¯ç”±'æˆ– TUN æ¨¡å¼)")
        return []

def fetch_hackernews_ai_posts() -> List[DataFetchFormatter]:
    """ä½¿ç”¨ Algolia æ¥å£ç²¾å‡†ç‹™å‡» Hacker News ä¸Šçš„ AI å‰æ²¿æƒ…æŠ¥"""
    logger.info("ğŸ“¡ æ­£åœ¨è°ƒç”¨ HN Algolia é›·è¾¾ï¼Œæ‰«æé«˜ä»·å€¼ AI æƒ…æŠ¥...")
    
    url = "https://hn.algolia.com/api/v1/search"
    
    # æ ¸å¿ƒé­”æ³•åœ¨è¿™é‡Œï¼šç²¾å‡†çš„æŸ¥è¯¢å‚æ•°
    query = ["AI", "LLM", "OpenAI", "ChatGPT", "Machine Learning", "MoE", "Token", "Context Window"]
    params = {
        # 1. æŸ¥è¯¢è¯å…¨éƒ¨ç”¨ç©ºæ ¼éš”å¼€ï¼ˆä¸è¦åŠ  ORï¼‰
        "query": " ".join(query),
        # 2. é­”æ³•å‚æ•°ï¼šå‘Šè¯‰ Algolia è¿™äº›è¯æ˜¯â€œå¯é€‰â€çš„ï¼Œåªè¦å‘½ä¸­ä¸€ä¸ªå°±ç®—åŒ¹é…ï¼
        "optionalWords": ",".join(query), 
        # åªæœç´¢æ­£å¼çš„æ–‡ç« ï¼ˆè¿‡æ»¤æ‰è¯„è®ºï¼‰
        "tags": "story",
        # è¿‡æ»¤æ¡ä»¶ï¼šåªçœ‹ç‚¹èµæ•°å¤§äº 30 çš„ï¼ˆè¿‡æ»¤æ‰æ²¡äººçœ‹çš„åƒåœ¾è´´ï¼‰
        "numericFilters": "points>30",
        # æ¯æ¬¡å–å‰ 3 æ¡æœ€ç›¸å…³çš„
        "hitsPerPage": 3
    }
    logger.debug(f"ğŸ” æŸ¥è¯¢å‚æ•°: {params}")
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code != 200:
            logger.error(f"âŒ é›·è¾¾æ‰«æå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
            
        data = response.json()
        # logger.debug(f"ğŸ” åŸå§‹æ•°æ®: {data}")  # è¾“å‡ºåŸå§‹æ•°æ®çœ‹çœ‹ç»“æ„ï¼Œæ–¹ä¾¿è°ƒè¯•
        posts = []
        
        # è§£æè¿”å›çš„æ•°æ®
        for item in data['hits']:
            # æœ‰äº› HN å¸–å­åªæœ‰æ–‡å­—æ²¡æœ‰å¤–é“¾ï¼Œæˆ‘ä»¬å°½é‡æŠ“æœ‰å¤–é“¾çš„
            post = DataFetchFormatter(
                title=item.get('title'),
                url=item.get('url', f"https://news.ycombinator.com/item?id={item['objectID']}"),
                author=item.get('author', 'unknown'),
                num_comments=item.get('num_comments', 0),
                objectID=item.get('objectID', ''),
                updated_at=item.get('updated_at', '')
            )
            
            if post.title:
                posts.append(post)
                
        return posts
        
    except Exception as e:
        logger.exception(f"é›·è¾¾æ‰«æå¼‚å¸¸: {str(e)}")
        return []


if __name__ == "__main__":
    # ç›´æ¥æµ‹è¯•æŠ“å–å‡½æ•°
    posts = fetch_hackernews_ai_posts()
    for post in posts:
        print("POST:", post)